{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hadoop.apache.org/docs/r1.2.1/images/hadoop-logo.jpg\">\n",
    "\n",
    "# Sistema de ficheros HDFS: configuración del entorno de la asignatura.\n",
    "\n",
    "Como ya se ha visto en la parte de teoría el sistema de archivos Hadoop (HDFS) es una parte fundamental del entorno Big Data de Apache Hadoop. En esta práctica exploraremos cómo podemos interactuar desde la linea de comandos con el sistema de ficheros HDFS. El primer paso es abrir un terminal desde el JupyterLab. Una vez confeccionada correctamente el pedido mediante la terminal se ruega que ésta sea ejecutada dentro del entorno JupyterLab como enseña a continuación.\n",
    "\n",
    "Los comandos que se pueden enviar al sistema de archivos son muy similares a las de bash en entornos Linux.\n",
    "\n",
    "Para listar los archivos del directorio raíz del HDFS utilizaremos el comando *ls*. \n",
    "\n",
    "`hdfs dfs -ls /`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan.ipynb\n",
      "JuanC\n",
      "Noelia.ipynb\n",
      "Raquel.ipynb\n",
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "Untitled2.ipynb\n",
      "Untitled3.ipynb\n",
      "edu.ipynb\n",
      "eduardo\n",
      "egomez\n",
      "jorgef\n",
      "marco.ipynb\n",
      "maria\n",
      "mrusso\n",
      "noelia\n",
      "pra01_HDFS-Copy1.ipynb\n",
      "pra01_HDFS.ipynb\n",
      "raquel\n",
      "rodri\n"
     ]
    }
   ],
   "source": [
    "# Listamos los ficheros en local\n",
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /JuanC\n",
      "drwxrwxrwt   - root     hadoop          0 2021-02-11 16:39 /hbase\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:39 /maria\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /mrusso\n",
      "drwxr-xr-x   - root     hadoop          0 2021-02-11 21:53 /neoland\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /noelia\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /rodri\n",
      "drwxrwxrwt   - hdfs     hadoop          0 2021-02-11 16:40 /tmp\n",
      "drwxrwxrwt   - hdfs     hadoop          0 2021-02-11 21:43 /user\n"
     ]
    }
   ],
   "source": [
    "# Listamos los ficheros en hadoop\n",
    "%%bash\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /JuanC\n",
      "drwxrwxrwt   - root     hadoop          0 2021-02-11 16:39 /hbase\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:39 /maria\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /mrusso\n",
      "drwxr-xr-x   - root     hadoop          0 2021-02-11 21:53 /neoland\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /noelia\n",
      "-rw-r--r--   2 mrussorb hadoop       5836 2021-02-12 16:37 /rodri\n",
      "drwxrwxrwt   - hdfs     hadoop          0 2021-02-11 16:40 /tmp\n",
      "drwxrwxrwt   - hdfs     hadoop          0 2021-02-12 17:28 /user\n"
     ]
    }
   ],
   "source": [
    "# es lo mismo que el comando anterior pero utilizando la !\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y para listar los ficheros del directorio /data: \n",
    "\n",
    "`hdfs dfs -ls /aula_B0.485`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/neoland/data/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /neoland/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root hadoop          0 2021-02-11 21:45 /neoland/labs\n",
      "-rw-r--r--   2 root hadoop       5836 2021-02-11 21:48 /neoland/pra01_HDFS.ipynb\n",
      "-rw-r--r--   2 root hadoop      29344 2021-02-11 21:53 /neoland/pra02_SPARK_WordCount.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Lo mismo que antes con los nombres de filesystem de hadoop mas largo\n",
    "!hadoop fs -ls /neoland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root hadoop          0 2021-02-11 21:45 /neoland/labs\n",
      "-rw-r--r--   2 root hadoop       5836 2021-02-11 21:48 /neoland/pra01_HDFS.ipynb\n",
      "-rw-r--r--   2 root hadoop      29344 2021-02-11 21:53 /neoland/pra02_SPARK_WordCount.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /neoland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se verá a continuación los comandos que HDFS proporciona, permiten: listar, consultar, subir o bajar archivos desde el directorio local en HDFS. Como ya se ha podido ver el aula contiene una serie de directorios donde se proporcionarán y se dejarán los archivos necesarios para la realización de la asignatura. A continuación se pide ejecutar una serie de operaciones que dejarán su directorio hdfs personal configurado para la realización de la asignatura. Puede encontrar el manual de los comandos del sistema HDFS en la parte de \"Contenidos y Recursos\" de la asignatura.\n",
    "\n",
    "1. Crear la carpeta \"data\" dentro del directorio HDFS */user/\\<loginestudiante\\>/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estoy creando el recurso en la carpeta jorge\n",
    "!hdfs dfs -mkdir /user/jorge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - mrussorb hadoop          0 2021-02-12 17:42 /user/jorge/data\n"
     ]
    }
   ],
   "source": [
    "# comprobamos que el recurso se ha creado correctamente\n",
    "!hdfs dfs -ls /user/jorge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Copiar al directorio \"data\" que acabais de crear el fichero \"Folds5x2_pp.csv\" que se encuentra en el directorio HDFS */neoland/labs/*. Utilitzad el comando `hdfs dfs -cp` para copiar el fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   2 root hadoop     318264 2021-02-11 17:18 /neoland/labs/Folds5x2_pp.csv\n",
      "-rw-r--r--   2 root hadoop  203431368 2021-02-11 17:12 /neoland/labs/LoremIpsum.txt\n",
      "drwxr-xr-x   - root hadoop          0 2021-02-11 21:46 /neoland/labs/pra02\n",
      "drwxr-xr-x   - root hadoop          0 2021-02-11 17:20 /neoland/labs/tweets28a_sample.json\n"
     ]
    }
   ],
   "source": [
    "# Listamos el directorio neoland labs\n",
    "!hdfs dfs -ls /neoland/labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: `/user/jorge/data/Folds5x2_pp.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# Realizamos la copia del archivo, en primer lugar lo ubicamos y luego le damos los parámetros y la ubicación donde lo almacenaremos\n",
    "!hdfs dfs -cp /neoland/labs/Folds5x2_pp.csv /user/jorge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 mrussorb hadoop     318264 2021-02-12 17:50 /user/jorge/data/Folds5x2_pp.csv\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos que hemos copiado de manera satisfactoria el archivo\n",
    "!hdfs dfs -ls /user/jorge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Copiar las 10 primeras lineas del fichero \"LoremIpsum.txt\" que se encuentra en el directorio HDFS */aula_B0.485/data/* a un nuevo fichero \"LoremIpsum_10.txt\" que creareis en el directorio HDFS */user/\\<loginestudiante\\>/data*. Os seran de utilidad los comandos `hdfs dfs -cat` i `hdfs dfs -put` a parte del comando classico de bash `head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Copiamos las primeras diez lineas y las almacenamos en un nuevo fichero\n",
    "!hdfs dfs -cat /neoland/labs/LoremIpsum.txt | head -10 | hdfs dfs -put - /user/jorge/data/LoremIpsum10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 mrussorb hadoop     318264 2021-02-12 17:50 /user/jorge/data/Folds5x2_pp.csv\n",
      "-rw-r--r--   2 mrussorb hadoop        935 2021-02-12 17:58 /user/jorge/data/LoremIpsum10.txt\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos que se generó el fichero correctamente\n",
    "!hdfs dfs -ls /user/jorge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aut minima deleniti et autem minus illo esse. Dolores eligendi corrupti dolore minima. Nostrum eos nobis nam nihil aspernatur nam. Ut quae sint laborum ut dolores. Error possimus aperiam consequatur.\n",
      "\n",
      "Pariatur sed quo non itaque qui pariatur saepe ad. Quis consequatur nihil iste molestias et eos ut. Expedita vel reiciendis dolorem. Enim doloribus quam architecto aperiam.\n",
      "\n",
      "Sed repudiandae pariatur similique est aut. Sequi animi in aperiam enim ipsa. Enim dolorem inventore aut quo odio in consequatur et.\n",
      "\n",
      "Aspernatur ad esse et aliquid itaque. Dolores rerum quia commodi explicabo non magnam. Nostrum consectetur non sint eum nulla et aut quis. Doloribus itaque nulla molestiae quis est est quo. Facilis incidunt a ipsa in itaque sed aut nobis. Facere dignissimos atque unde cum ea vero.\n",
      "\n",
      "Tenetur vel quod voluptatum laudantium dolores neque. Aut est modi qui aperiam itaque aperiam quae. Ratione doloremque aut delectus quas qui.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos que se han escrito las diez líneas en el archivo\n",
    "!hdfs dfs -cat /user/jorge/data/LoremIpsum10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Descargar el notebook de python de la primera práctica que encontrareis en el directorio HDFS */aula_B0.485/PECs* . Utilizar el comando `hdfs dfs -get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!<insert_command>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parte de los comandos que ya habéis visto orientados a que cada usuario pueda gestionar el sistema de fitxeros HDFS, también es posible explorar la organización interna de los diferentes fitxeros existentes en los directorios HDFS.\n",
    "\n",
    "5. Se pide que mediante el comando `hdfs fsck` se determine i describa como esta estructurado el fichero _/neoland/data/LoremIpsum.txt_ dentro del sistema HDFS. En concreto se pide mostrar como está subdividido en bloques, en que nodo están los bloques y si estos estéan replicados. Podeis consultar està información en la web oficial de referéncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-02-21 pra01_HDFS_jorgef.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop-grp1-m:9870/fsck?ugi=jfdacovich&blocks=1&path=%2Fneoland%2Flabs%2FLoremIpsum.txt\n",
      "FSCK started by jfdacovich (auth:SIMPLE) from /10.128.0.11 for path /neoland/labs/LoremIpsum.txt at Sat Feb 13 08:34:50 UTC 2021\n",
      ".Status: HEALTHY\n",
      " Total size:\t203431368 B\n",
      " Total dirs:\t0\n",
      " Total files:\t1\n",
      " Total symlinks:\t\t0\n",
      " Total blocks (validated):\t2 (avg. block size 101715684 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Number of data-nodes:\t\t2\n",
      " Number of racks:\t\t1\n",
      "FSCK ended at Sat Feb 13 08:34:50 UTC 2021 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/neoland/labs/LoremIpsum.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "#!<insert_command>\n",
    "#!hdfs fsck LoremImpsum.txt -blocks -locations\n",
    "#!hdfs fsck -ls /neoland/labs/LoremImpsum.txt -blocks -locations\n",
    "#!hdfs fsck neoland/labs/LoremImpsum.txt [-blocks |-locations]\n",
    "!hdfs fsck /neoland/labs/LoremIpsum.txt -blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop-grp1-m:9870/fsck?ugi=jfdacovich&files=1&blocks=1&racks=1&path=%2Fneoland%2Flabs%2FLoremIpsum.txt\n",
      "FSCK started by jfdacovich (auth:SIMPLE) from /10.128.0.11 for path /neoland/labs/LoremIpsum.txt at Sat Feb 13 08:57:00 UTC 2021\n",
      "/neoland/labs/LoremIpsum.txt 203431368 bytes, 2 block(s):  OK\n",
      "0. BP-1027074765-10.128.0.11-1613061490296:blk_1073741837_1013 len=134217728 Live_repl=2 [/default-rack/10.128.0.12:9866, /default-rack/10.128.0.13:9866]\n",
      "1. BP-1027074765-10.128.0.11-1613061490296:blk_1073741838_1014 len=69213640 Live_repl=2 [/default-rack/10.128.0.12:9866, /default-rack/10.128.0.13:9866]\n",
      "\n",
      "Status: HEALTHY\n",
      " Total size:\t203431368 B\n",
      " Total dirs:\t0\n",
      " Total files:\t1\n",
      " Total symlinks:\t\t0\n",
      " Total blocks (validated):\t2 (avg. block size 101715684 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Number of data-nodes:\t\t2\n",
      " Number of racks:\t\t1\n",
      "FSCK ended at Sat Feb 13 08:57:00 UTC 2021 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/neoland/labs/LoremIpsum.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /neoland/labs/LoremIpsum.txt -files -blocks -racks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop-grp1-m:9870/fsck?ugi=jfdacovich&locations=1&path=%2Fneoland%2Flabs%2FLoremIpsum.txt\n",
      "FSCK started by jfdacovich (auth:SIMPLE) from /10.128.0.11 for path /neoland/labs/LoremIpsum.txt at Sat Feb 13 08:36:04 UTC 2021\n",
      ".Status: HEALTHY\n",
      " Total size:\t203431368 B\n",
      " Total dirs:\t0\n",
      " Total files:\t1\n",
      " Total symlinks:\t\t0\n",
      " Total blocks (validated):\t2 (avg. block size 101715684 B)\n",
      " Minimally replicated blocks:\t2 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Number of data-nodes:\t\t2\n",
      " Number of racks:\t\t1\n",
      "FSCK ended at Sat Feb 13 08:36:04 UTC 2021 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/neoland/labs/LoremIpsum.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /neoland/labs/LoremIpsum.txt -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Como sabeis los diferentes ficheros HDFS están particionados en bloques. Podeis consultar la configuración mediante el comando: `hdfs getconf -confKey dfs.blocksize`. Quantos MB hay por bloque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       8.0 E           8.0 E            none             inf           71          244            211.5 M /\n"
     ]
    }
   ],
   "source": [
    "#Filesystem              Size    Used  Available  Use%\n",
    "!hadoop fs -count -q -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey dfs.blocksize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
